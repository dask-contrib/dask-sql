---
# Test the main branch and every pull request by
# 1. building the jar on ubuntu
# 2. testing code (using the build jar) on ubuntu and windows, with different java versions
name: Test
on:
  push:
    branches:
      - main
  pull_request:

# When this workflow is queued, automatically cancel any previous running
# or pending jobs from the same branch
concurrency:
  group: test-${{ github.head_ref }}
  cancel-in-progress: true

jobs:
  build:
    # This build step should be similar to the deploy build, to make sure we actually test
    # the future deployable
    name: "Build the jar on ubuntu"
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Cache local Maven repository
        uses: actions/cache@v2
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-v1-jdk11-${{ hashFiles('**/pom.xml') }}
      - name: Set up Python
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-variant: Mambaforge
          use-mamba: true
          python-version: 3.8
          activate-environment: dask-sql
          environment-file: continuous_integration/environment-3.8-jdk11-dev.yaml
      - name: Install dependencies and build the jar
        shell: bash -l {0}
        run: |
          python setup.py java
      - name: Upload the jar
        uses: actions/upload-artifact@v1
        with:
          name: jar
          path: dask_sql/jar/DaskSQL.jar

  test:
    name: "Test (${{ matrix.os }}, java: ${{ matrix.java }}, python: ${{ matrix.python }})"
    needs: build
    runs-on: ${{ matrix.os }}
    env:
      CONDA_FILE: continuous_integration/environment-${{ matrix.python }}-jdk${{ matrix.java }}-dev.yaml
    strategy:
      matrix:
        java: [8, 11]
        os: [ubuntu-latest, windows-latest]
        python: [3.7, 3.8]
    steps:
      - uses: actions/checkout@v2
      - name: Cache local Maven repository
        uses: actions/cache@v2
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-v1-jdk${{ matrix.java }}-${{ hashFiles('**/pom.xml') }}
      - name: Set up Python
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-variant: Mambaforge
          use-mamba: true
          python-version: ${{ matrix.python }}
          activate-environment: dask-sql
          environment-file: ${{ env.CONDA_FILE }}
      - name: Download the pre-build jar
        uses: actions/download-artifact@v1
        with:
          name: jar
          path: dask_sql/jar/
      - name: Install hive testing dependencies for Linux
        shell: bash -l {0}
        run: |
          mamba install -c conda-forge sasl>=0.3.1
          docker pull bde2020/hive:2.3.2-postgresql-metastore
          docker pull bde2020/hive-metastore-postgresql:2.3.0
        if: matrix.os == 'ubuntu-latest'
      - name: Set proper JAVA_HOME for Windows
        shell: bash -l {0}
        run: |
          echo "JAVA_HOME=${{ env.CONDA }}\envs\dask-sql\Library" >> $GITHUB_ENV
        if: matrix.os == 'windows-latest'
      - name: Test with pytest
        shell: bash -l {0}
        run: |
          pytest --junitxml=junit/test-results.xml --cov-report=xml -n auto tests --dist loadfile
      - name: Upload pytest test results
        uses: actions/upload-artifact@v1
        with:
          name: pytest-results
          path: junit/test-results.xml
        # Use always() to always run this step to publish test results when there are test failures
        if: ${{ always() }}
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v2
        with:
          fail_ci_if_error: true
          token: ${{ secrets.CODECOV_TOKEN }}

  cluster:
    name: "Test in a dask cluster"
    needs: build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Cache local Maven repository
        uses: actions/cache@v2
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-v1-jdk11-${{ hashFiles('**/pom.xml') }}
      - name: Set up Python
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-variant: Mambaforge
          use-mamba: true
          python-version: 3.8
          activate-environment: dask-sql
          environment-file: continuous_integration/environment-3.8-jdk11-dev.yaml
      - name: Download the pre-build jar
        uses: actions/download-artifact@v1
        with:
          name: jar
          path: dask_sql/jar/
      - name: Install dependencies
        shell: bash -l {0}
        run: |
          mamba install python-blosc lz4 -c conda-forge

          which python
          pip list
          mamba list
      - name: run a dask cluster
        shell: bash -l {0}
        run: |
          docker-compose -f .github/docker-compose.yaml up -d

          # Wait for installation
          sleep 40

          docker logs dask-scheduler
          docker logs dask-worker
      - name: Test with pytest while running an independent dask cluster
        shell: bash -l {0}
        run: |
          pytest tests
        env:
          DASK_SQL_TEST_SCHEDULER: tcp://127.0.0.1:8786

  import:
    name: "Test importing with bare requirements"
    needs: build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Cache local Maven repository
        uses: actions/cache@v2
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-v1-jdk11-${{ hashFiles('**/pom.xml') }}
      - name: Set up Python
        uses: conda-incubator/setup-miniconda@v2
        with:
          python-version: 3.8
          mamba-version: "*"
          channels: conda-forge,defaults
          channel-priority: true
      - name: Download the pre-build jar
        uses: actions/download-artifact@v1
        with:
          name: jar
          path: dask_sql/jar/
      - name: Install dependencies and nothing else
        shell: bash -l {0}
        run: |
          pip install -e .

          which python
          pip list
          mamba list
      - name: Try to import dask-sql
        shell: bash -l {0}
        run: |
          python -c "import dask_sql; print('ok')"

  conda:
    name: "Test building the conda package"
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
        with:
          fetch-depth: 0
      - name: Set up Python
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-variant: Mambaforge
          use-mamba: true
          python-version: 3.8
      - name: Install dependencies
        shell: bash -l {0}
        run: |
          mamba install conda-build conda-verify

          which python
          pip list
          mamba list
      - name: Build conda package
        shell: bash -l {0}
        run: |
          # suffix for nightly package versions
          export VERSION_SUFFIX=`date +%y%m%d`

          mamba build continuous_integration/recipe \
                      --no-anaconda-upload \
                      --output-folder .
      - name: Upload conda package
        if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' && github.repository == 'dask-contrib/dask-sql' }}
        shell: bash -l {0}
        env:
          ANACONDA_API_TOKEN: ${{ secrets.DASK_CONDA_TOKEN }}
        run: |
          # install anaconda for upload
          mamba install anaconda-client

          anaconda upload --label dev noarch/*.tar.bz2
